{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0488cae-f2b2-4d0f-9e42-7cd4faae07d8",
   "metadata": {},
   "source": [
    "# Deploy NVIDIA Inference Microservice (NIM) on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf191c-ac98-4d56-a6e5-a43e6de87b13",
   "metadata": {},
   "source": [
    "NVIDIA NIM for LLMs brings the power of state-of-the-art large language models (LLM) to your applications, providing unmatched natural language processing and understanding capabilities. Whether you’re developing chatbots, content analyzers—or any application that needs to understand and generate human language—NVIDIA NIM for LLMs has you covered. Built on the NVIDIA software platform incorporating CUDA, TRT, TRT-LLM, and Triton, NVIDIA NIM for LLMs brings state of the art GPU accelerated Large Language model serving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1048b66-14f3-4f47-91fd-e653979c7cd5",
   "metadata": {},
   "source": [
    "In this example we show how to deploy `LLaMa-2 7B` model with NIM on Amazon SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa383fca-0ffb-45f9-a6cf-1849d117a386",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3686a50f-24d5-4778-a02d-28efc31373b7",
   "metadata": {},
   "source": [
    "Installs the dependencies and setup roles required to package the model and create SageMaker endpoint. Also define the NIM Image URI we will be using for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7578a7de-7ed3-4105-bec7-e5d3b04cd4bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3, json, sagemaker, time\n",
    "from sagemaker import get_execution_role\n",
    "from pathlib import Path\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role = get_execution_role()\n",
    "client = boto3.client(\"sagemaker-runtime\")\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411d6a77-cc80-4eac-b547-33b68ec5e878",
   "metadata": {},
   "source": [
    "We define the NIM image URI that we will be using for deploying on SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83ea3a7-2f27-4d75-aab9-00bb50b438f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nim_image_uri = \"354625738399.dkr.ecr.us-east-1.amazonaws.com/nim-24.02-sm-final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90589a2b-2314-4ba6-9f6b-a91a6b0d4093",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nim_image_uri = \"<ACCOUNT>.dkr.ecr.<REGION>.amazonaws.com/nim-24.02-sm-final\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accb7611-819a-4be8-b4d5-c1b3af59f88f",
   "metadata": {},
   "source": [
    "### Packaging model and uploading to s3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e5b107-3240-4710-ba63-217a5a09f223",
   "metadata": {},
   "source": [
    "The prebuilt model engine we downloaded from NGC is already in `.tar.gz` format so we will upload it to s3 to later create a SageMaker Model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e2af3a-7c46-4db5-b085-5f2988ce3a72",
   "metadata": {},
   "source": [
    "First we set the path to downloaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62658145-2812-4a53-b096-e4cb69ef2adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DOWNLOADED_PATH = 'llama-2-7b-chat_vLLAMA-2-7B-CHAT-4K-FP16-1-A100.24.02.rc2/LLAMA-2-7B-CHAT-4K-FP16-1-A100.24.02.rc2.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8825c1e-a165-494e-ab9b-3a73e59c3258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set me\n",
    "#MODEL_DOWNLOADED_PATH = <MODEL PATH>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5360bb55-7269-48b0-9751-6cbeeec0fd2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "current_directory = Path.cwd()\n",
    "path = current_directory / MODEL_DOWNLOADED_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96626f35-c446-4eb7-9f71-2f09fd98ee6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_uri = sagemaker_session.upload_data(path=path, key_prefix=\"nim-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2518de4e-dcad-4944-9025-484878edb00b",
   "metadata": {},
   "source": [
    "### Create SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d76a46-085e-4bc0-9c8e-9d633768bbcf",
   "metadata": {},
   "source": [
    "Next we can start creating a sagemaker model from the model we uploaded to s3 in the previous step.\n",
    "\n",
    "In this step we also need to provide additional Environment Variables\n",
    "- `SAGEMAKER_MODEL_NAME` which specifies the name of the model to be loaded by NIM container on SageMaker. You can provide any name, you just have to make sure it matches the name you provide in the inference request also.\n",
    "- `SAGEMAKER_NUM_GPUS` which specifies the number of GPUs the model was prebuilt to run inference on. This was specified in the name of the prebuilt model engine you downloaded from NGC. For example if the model is LLAMA-2-7B-CHAT-4K-FP16-1-A100.24.02 then that means it's LLama 2 7B Chat model with 4K context len, optimized for `FP16` precision and designed to run on 1 A100 GPU. Similarly, if it's `LLAMA-2-70B-CHAT-4K-FP16-4-A100-24.02` then that means it's LLama 2 70B Chat model with 4K context len, optimized for FP16 precision and designed to run on 4-A100 GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3ffb98-998b-47fe-97d0-e5e80976ef48",
   "metadata": {},
   "source": [
    "Here we set model name as `llama-2-7b` and `num of GPUs as 1` because that's what our example prebuilt engine is built for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707b5450-78cd-4c90-bfda-59141f162777",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAGEMAKER_MODEL_NAME = \"llama-2-7b\"\n",
    "SAGEMAKER_NUM_GPUS = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b784149-2ec3-4e29-a7cf-3636843dee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = {\n",
    "    \"Image\": nim_image_uri,\n",
    "    \"ModelDataUrl\": model_uri,\n",
    "    \"Environment\": {\"SAGEMAKER_MODEL_NAME\": SAGEMAKER_MODEL_NAME,\n",
    "                    \"SAGEMAKER_NUM_GPUS\": SAGEMAKER_NUM_GPUS}\n",
    "}\n",
    "sm_prefix = \"nim-model-\"\n",
    "\n",
    "sm_model_name = sm_prefix + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d658fa44-7d89-4eae-8667-d0ab8a2f47e0",
   "metadata": {},
   "source": [
    "Using the model above, we create an endpoint configuration where we can specify the type of instance we want in the endpoint. \n",
    "\n",
    "**IMPORTANT: In this case since prebuilt engine was optimized for A100 GPU we specify the `Instance Type` as `ml.p4d.24xlarge`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0af8b7c-9347-4203-aea5-f44392449f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-1:354625738399:endpoint-config/symlink-nim-llama-2-7b-a100-2024-03-12-01-28-48\n"
     ]
    }
   ],
   "source": [
    "endpoint_config_name = sm_prefix + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.p4d.24xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e51121a-a662-4078-a0c6-b163cda0a718",
   "metadata": {},
   "source": [
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to InService once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75add3d0-100f-4740-b326-6f54af7e9c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Arn: arn:aws:sagemaker:us-east-1:354625738399:endpoint/symlink-nim-llama-2-7b-a100-2024-03-12-01-28-49\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = sm_prefix + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec2d4bc4-b77b-4137-930e-7517295a041c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-east-1:354625738399:endpoint/symlink-nim-llama-2-7b-a100-2024-03-12-01-28-49\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a97e4c-6dd8-4d9a-841c-e443b7c1583f",
   "metadata": {},
   "source": [
    "### Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9146f44-b85c-4125-b842-fceaf5c3cfa8",
   "metadata": {},
   "source": [
    "TODO need to add explanations for all these"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b71bf-8444-4265-bc56-536d8b4380d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "Supported parameters for inference include\n",
    "\n",
    "* model\n",
    "* prompt\n",
    "* frequency_penalty\n",
    "* max_tokens\n",
    "* n = 1\n",
    "* stop\n",
    "* stream\n",
    "* temperature\n",
    "* top_p\n",
    "* logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a57265e9-98bb-4255-ad7d-143e3aeaf9d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-4f77577f-24a3-4e70-bfec-1fcef3420720', 'object': 'text_completion', 'created': 1710207480, 'model': 'llama-2-7b', 'choices': [{'index': 0, 'text': \" Paris. It's a beautiful city with many famous landmarks, such as the Eiffel Tower and Notre Dame Cathedral.\\nParis has been home to some very important historical events in Europe including: 1789 French Revolution began here; Napoleon Bonaparte was crowned emperor at the cathedral (Notre-Dame)in year 1804 & ended his reign there after defeat by allied forces during Franco Prussian War(1\", 'logprobs': {'text_offset': [], 'token_logprobs': [0.0, 0.0], 'tokens': [], 'top_logprobs': []}}], 'usage': {'prompt_tokens': 7, 'total_tokens': 107, 'completion_tokens': 100}}\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "  \"model\": model_name,\n",
    "  \"prompt\": \"The capital of France is called\",\n",
    "  \"max_tokens\": 100,\n",
    "  \"temperature\": 1,\n",
    "  \"n\": 1,\n",
    "  \"stream\": False,\n",
    "  \"stop\": [\"string\"],\n",
    "  \"frequency_penalty\": 0.0\n",
    "}\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=\"application/json\", Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "print(json.loads(response[\"Body\"].read().decode(\"utf8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fd10eb-4f06-4b81-a13b-1784750bda39",
   "metadata": {},
   "source": [
    "## Try streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fd68a0-9c54-4721-955c-61e4e3238137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "class LineIterator:\n",
    "    \"\"\"\n",
    "    A helper class for parsing the byte stream input. \n",
    "    \n",
    "    The output of the model will be in the following format:\n",
    "    ```\n",
    "    b'{\"outputs\": [\" a\"]}\\n'\n",
    "    b'{\"outputs\": [\" challenging\"]}\\n'\n",
    "    b'{\"outputs\": [\" problem\"]}\\n'\n",
    "    ...\n",
    "    ```\n",
    "    \n",
    "    While usually each PayloadPart event from the event stream will contain a byte array \n",
    "    with a full json, this is not guaranteed and some of the json objects may be split across\n",
    "    PayloadPart events. For example:\n",
    "    ```\n",
    "    {'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n",
    "    {'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n",
    "    ```\n",
    "    \n",
    "    This class accounts for this by concatenating bytes written via the 'write' function\n",
    "    and then exposing a method which will return lines (ending with a '\\n' character) within\n",
    "    the buffer via the 'scan_lines' function. It maintains the position of the last read \n",
    "    position to ensure that previous bytes are not exposed again. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "            if line and line[-1] == ord('\\n'):\n",
    "                self.read_pos += len(line)\n",
    "                return line[:-1]\n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "            if 'PayloadPart' not in chunk:\n",
    "                print('Unknown event type:' + chunk)\n",
    "                continue\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk['PayloadPart']['Bytes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8491f0e-1eb1-4128-949a-c57b5848a705",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "  \"model\": \"llama-2-7b\",\n",
    "  \"prompt\": \"The capital of France is called\",\n",
    "  \"max_tokens\": 100,\n",
    "  \"temperature\": 1,\n",
    "  \"n\": 1,\n",
    "  \"stream\": True,\n",
    "  \"stop\": [\"string\"],\n",
    "  \"frequency_penalty\": 0.0\n",
    "}\n",
    "\n",
    "response = client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name, ContentType=\"application/json\", Body=json.dumps(payload)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f06345e-2500-4082-8d06-d1148e840d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_stream = response['Body'].read().decode(\"utf8\"))\n",
    "\n",
    "for line in LineIterator(event_stream):\n",
    "    resp = json.loads(line)\n",
    "    print(resp.get(\"outputs\")[0], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addeadcd-6ff8-4603-9f66-fe52e9f2cc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_stream = response['Body'])\n",
    "\n",
    "for line in LineIterator(event_stream):\n",
    "    resp = json.loads(line.read().decode(\"utf8\"))\n",
    "    print(resp.get(\"outputs\")[0], end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19063f6-b6c0-4de2-a193-e482f26f7406",
   "metadata": {},
   "source": [
    "## Terminate endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db083f-4705-4c68-a488-f82da961be4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm.delete_model(ModelName=sm_model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
