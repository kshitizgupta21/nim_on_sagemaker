{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0488cae-f2b2-4d0f-9e42-7cd4faae07d8",
   "metadata": {},
   "source": [
    "# Deploy NVIDIA NIM on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf191c-ac98-4d56-a6e5-a43e6de87b13",
   "metadata": {},
   "source": [
    "NVIDIA NIM, part of NVIDIA AI Enterprise, brings the power of state-of-the-art large language models (LLM) to your applications, providing unmatched natural language processing and understanding capabilities. Whether you’re developing chatbots, content analyzers—or any application that needs to understand and generate human language—NVIDIA NIM for LLMs has you covered. Built on the robust foundations including\n",
    "inference engines like Triton Inference Server, TensorRT, TensorRT-LLM, and PyTorch, NIM is\n",
    "engineered to facilitate seamless AI inferencing at scale, ensuring that you can deploy AI\n",
    "applications with confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d71ddd0-934b-433d-91d4-a6f3f34f3ef5",
   "metadata": {},
   "source": [
    "## Using Non-Prebuilt Models with NIM Model Repo Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1048b66-14f3-4f47-91fd-e653979c7cd5",
   "metadata": {},
   "source": [
    "In this example we show how you can use NIM's Model Repo Generator to create optimize your custom model and and deploy it with NIM on Amazon SageMaker on **g5.xlarge (A10G GPU)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa383fca-0ffb-45f9-a6cf-1849d117a386",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3686a50f-24d5-4778-a02d-28efc31373b7",
   "metadata": {},
   "source": [
    "Installs the dependencies and setup roles required to download the model, package the model and create SageMaker inference endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7578a7de-7ed3-4105-bec7-e5d3b04cd4bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3, json, sagemaker, time\n",
    "from sagemaker import get_execution_role\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role = get_execution_role()\n",
    "client = boto3.client(\"sagemaker-runtime\")\n",
    "region = sess.region_name\n",
    "sts_client = sess.client('sts')\n",
    "account_id = sts_client.get_caller_identity()['Account']\n",
    "\n",
    "def create_directory(directory_path):\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "        print(f\"Directory '{directory_path}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Directory '{directory_path}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411d6a77-cc80-4eac-b547-33b68ec5e878",
   "metadata": {},
   "source": [
    "We define the NIM image URI that we will be using for deploying on SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83ea3a7-2f27-4d75-aab9-00bb50b438f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nim_image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/nim-<YY>.<MM>-sm\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0738ba9-955f-4c8d-bfd1-6695595b9ac8",
   "metadata": {},
   "source": [
    "### Download Llama Model From HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30f416c-ebf1-4f83-b7ac-84e44bb0994d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install git lfs\n",
    "!sudo amazon-linux-extras install epel -y\n",
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash\n",
    "!sudo yum install git-lfs -y\n",
    "!git lfs install\n",
    "!git clone https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1172588d-b59b-4682-998a-6ede09865343",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "current_directory = Path.cwd()\n",
    "hf_model_path = current_directory / \"TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582af0c9-0ab4-4b62-8de0-318e31a4019c",
   "metadata": {},
   "source": [
    "### Creating the Model Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de40a64-60e4-49c0-b84c-008e6c92a3a5",
   "metadata": {},
   "source": [
    "First, create a model_config.yaml file. For `Llama` model it should look like the following. You can edit `max_batch_size`, `max_input_len`, `max_output_len` if you want. To build the engine for multi-GPU inference you can edit `num_gpus` and `tensor_para_size`. Here we just go wtih default settings and build the engine for 1-GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c665a76-38d5-4cdb-ab8e-224953bc4cc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile model_config.yaml\n",
    "model_repo_path: \"/model-store/\"\n",
    "use_ensemble: false\n",
    "model_type: \"LLAMA\"\n",
    "backend: \"trt_llm\"\n",
    "base_model_id: \"ensemble\"\n",
    "prompt_timer: 60\n",
    "gateway_ip: \"gateway-api\"\n",
    "customization_cache_capacity: 10000\n",
    "logging_level: \"INFO\"\n",
    "enable_chat: true\n",
    "pipeline:\n",
    "  model_name: \"ensemble\"\n",
    "  num_instances: 1\n",
    "trt_llm:\n",
    "  use: true\n",
    "  ckpt_type: \"hf\"\n",
    "  model_name: \"trt_llm\"\n",
    "  backend: \"python\"\n",
    "  num_gpus: 1\n",
    "  model_path: /engine_dir\n",
    "  max_queue_delay_microseconds: 10000\n",
    "  model_type: \"llama\"\n",
    "  max_batch_size: 1\n",
    "  max_input_len: 256\n",
    "  max_output_len: 256\n",
    "  max_beam_width: 1\n",
    "  tensor_para_size: 1\n",
    "  pipeline_para_size: 1\n",
    "  data_type: \"float16\"\n",
    "  int8_mode: 0\n",
    "  enable_custom_all_reduce: 0\n",
    "  per_column_scaling: false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5f7ca2-b4ea-435a-9f32-ff5da617582c",
   "metadata": {},
   "source": [
    "### Creating an Empty Directory for model-store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d390c-7d52-4c7a-b5ad-d2d4c83ab593",
   "metadata": {},
   "source": [
    "The model store directory must be empty because the output from the Model Repo Generator command is stored there. So we create empty `model-store` directory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612096c1-8766-472f-b632-fed5b56c1949",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = \"model-store\"\n",
    "create_directory(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9104a64b-e6d4-4807-88e4-d99a3cf08fc7",
   "metadata": {},
   "source": [
    "### Running the Model Repo Generator Command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e3187e-3ded-4121-b955-c0c8bd270446",
   "metadata": {},
   "source": [
    "The following command generates the model repository in the specified path for `model-store`. We must also pass in the location of the `model_config.yaml` file we created and pass in the path to the `Llama` model that we downloaded from HuggingFace and expose it in the container as the value for `/engine_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f62429-af4a-45c5-a5c0-7318f10b327b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir tmp\n",
    "!docker run --rm -it --gpus all -v $(pwd)/model-store:/model-store -v $(pwd)/model_config.yaml:/model_config.yaml -v {hf_model_path}/:/engine_dir -v $(pwd)/tmp:/tmp {nim_image_uri}  bash -c \"model_repo_generator llm --verbose --yaml_config_file=/model_config.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accb7611-819a-4be8-b4d5-c1b3af59f88f",
   "metadata": {},
   "source": [
    "### Packaging model and uploading to s3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f658247-66e3-4bf7-9f18-48fab00950ba",
   "metadata": {},
   "source": [
    "SageMaker expects `tar.gz` format for the model artifact so we need to package our model and then we upload it to S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5660743e-a273-493a-b696-af6bb4f67dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -czf model.tar.gz model-store\n",
    "model_uri = sagemaker_session.upload_data(path=\"model.tar.gz\", key_prefix=\"nim-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2518de4e-dcad-4944-9025-484878edb00b",
   "metadata": {},
   "source": [
    "### Create SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d76a46-085e-4bc0-9c8e-9d633768bbcf",
   "metadata": {},
   "source": [
    "Next we can start creating a sagemaker model from the model we uploaded to s3 in the previous step.\n",
    "\n",
    "In this step we also need to provide additional Environment Variables\n",
    "- `SAGEMAKER_MODEL_NAME` which specifies the name of the model to be loaded by NIM container on SageMaker. You can provide any name, you just have to make sure it matches the name you provide in the inference request also.\n",
    "- `SAGEMAKER_NUM_GPUS` which specifies the number of GPUs the model was prebuilt to run inference on. This was specified in the name of the prebuilt model engine you downloaded from NGC. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3ffb98-998b-47fe-97d0-e5e80976ef48",
   "metadata": {},
   "source": [
    "Here we set model name as `llama` and `num of GPUs = 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707b5450-78cd-4c90-bfda-59141f162777",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SAGEMAKER_MODEL_NAME = \"llama\"\n",
    "SAGEMAKER_NUM_GPUS = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b784149-2ec3-4e29-a7cf-3636843dee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = {\n",
    "    \"Image\": nim_image_uri,\n",
    "    \"ModelDataUrl\": model_uri,\n",
    "    \"Environment\": {\"SAGEMAKER_MODEL_NAME\": SAGEMAKER_MODEL_NAME,\n",
    "                    \"SAGEMAKER_NUM_GPUS\": SAGEMAKER_NUM_GPUS}\n",
    "}\n",
    "sm_prefix = \"nim-model-\" + SAGEMAKER_MODEL_NAME\n",
    "\n",
    "sm_model_name = sm_prefix + time.strftime(\"-%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d658fa44-7d89-4eae-8667-d0ab8a2f47e0",
   "metadata": {},
   "source": [
    "Using the model above, we create an endpoint configuration where we can specify the type of instance we want in the endpoint. \n",
    "\n",
    "**IMPORTANT: In this case since we used NIM's Model Repo Generator to build the model engine for g5 (A10G) GPU we need to specify the `Instance Type` as `ml.g5.xlarge`**\n",
    "\n",
    "**Engine that is built for certain GPU should only be deployed with that specific GPU. For example, if you want to deploy on p4d.24xlarge (A100 40GB) then you should run Model Repo Generator with your model in p4d.24xlarge notebook instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0af8b7c-9347-4203-aea5-f44392449f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_config_name = sm_prefix + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g5.xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e51121a-a662-4078-a0c6-b163cda0a718",
   "metadata": {},
   "source": [
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to InService once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75add3d0-100f-4740-b326-6f54af7e9c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = sm_prefix + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2d4bc4-b77b-4137-930e-7517295a041c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a97e4c-6dd8-4d9a-841c-e443b7c1583f",
   "metadata": {},
   "source": [
    "### Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9146f44-b85c-4125-b842-fceaf5c3cfa8",
   "metadata": {},
   "source": [
    "Once we have the endpoint running we can use a sample text to do a prompt text completion inference request using json as the payload format. For inference request format, NIM on SageMaker supports the OpenAI API /completions inference protocol. For explanation of supported parameters please see [this link](https://platform.openai.com/docs/api-reference/completions/create). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57265e9-98bb-4255-ad7d-143e3aeaf9d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "  \"model\": SAGEMAKER_MODEL_NAME,\n",
    "  \"prompt\": \"<|system|> You are a chatbot who can help code!</s> <|user|> Write me a function to calculate the first 10 digits of the fibonacci sequence in Python and print it out to the CLI.</s> <|assistant|>\",\n",
    "  \"max_tokens\": 100,\n",
    "  \"temperature\": 1,\n",
    "  \"n\": 1,\n",
    "  \"stream\": False,\n",
    "  \"stop\": [\"string\"],\n",
    "  \"frequency_penalty\": 0.0\n",
    "}\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=\"application/json\", Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "print(json.dumps(output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3308dfa3-be2b-48ae-a530-33ca60091f27",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Try streaming inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9981af9e-921c-49ed-90b1-f7d92e4d85fa",
   "metadata": {},
   "source": [
    "NIM on SageMaker also supports streaming inference and you can enable that by setting **`\"stream\"` as `True`** in the payload and by using [`invoke_endpoint_with_response_stream`](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime/client/invoke_endpoint_with_response_stream.html) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b658ce8-c993-40f9-8bd2-c0cdd1fab8d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "  \"model\": SAGEMAKER_MODEL_NAME,\n",
    "  \"prompt\": \"<|system|> You are a chatbot who can help code!</s> <|user|> Write me a function to calculate the first 10 digits of the fibonacci sequence in Python and print it out to the CLI.</s> <|assistant|>\",\n",
    "  \"max_tokens\": 100,\n",
    "  \"temperature\": 1,\n",
    "  \"n\": 1,\n",
    "  \"stream\": True,\n",
    "  \"stop\": [\"string\"],\n",
    "  \"frequency_penalty\": 0.0\n",
    "}\n",
    "response = client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(payload),\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/jsonlines\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04c1737-fa58-4e29-9bd5-c82f7b863835",
   "metadata": {},
   "source": [
    "We do some postprocessing on the event stream to handle the streaming output tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a2d41d-f037-4601-9d08-0067bc21ded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import LineIterator, has_same_prefix\n",
    "import re\n",
    "\n",
    "event_stream = response[\"Body\"]\n",
    "\n",
    "# Create an instance of LineIterator\n",
    "line_iterator = LineIterator(event_stream)\n",
    "\n",
    "# Iterate over the lines\n",
    "prev = None\n",
    "for line in line_iterator:\n",
    "    # Decode the line into bytes\n",
    "    decoded_line = line[len(b'data: '):].decode(\"utf-8\").rstrip('\\n')\n",
    "\n",
    "    if decoded_line == \" [DONE]\":\n",
    "        print(prev)\n",
    "        print(\"\\nStreaming Generation Finished!\")\n",
    "        break\n",
    "    else:\n",
    "        # Extract the desired information from the JSON\n",
    "        decoded_json = json.loads(decoded_line)\n",
    "        text = decoded_json['choices'][0]['text']\n",
    "        # print(text)\n",
    "        words_and_punctuations = re.findall(r\"[\\w']+|[.,!?;&()\\\"–—:;!*#@$%/\\\\<>\\[\\]{}|^~=+]\", text)# Get the last word\n",
    "        # print(words_and_punctuations[-1])\n",
    "        # print(\"===========\")\n",
    "        if len(words_and_punctuations) > 0:\n",
    "            if not has_same_prefix(prev, words_and_punctuations[-1]) and prev is not None:\n",
    "                # print(\"**************\")\n",
    "                print(prev, end=' ')\n",
    "            prev = words_and_punctuations[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19063f6-b6c0-4de2-a193-e482f26f7406",
   "metadata": {},
   "source": [
    "### Terminate endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db083f-4705-4c68-a488-f82da961be4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm.delete_model(ModelName=sm_model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
