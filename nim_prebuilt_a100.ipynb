{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0488cae-f2b2-4d0f-9e42-7cd4faae07d8",
   "metadata": {},
   "source": [
    "# Deploy NVIDIA Inference Microservice (NIM) on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf191c-ac98-4d56-a6e5-a43e6de87b13",
   "metadata": {},
   "source": [
    "NVIDIA NIM for LLMs brings the power of state-of-the-art large language models (LLM) to your applications, providing unmatched natural language processing and understanding capabilities. Whether you’re developing chatbots, content analyzers—or any application that needs to understand and generate human language—NVIDIA NIM for LLMs has you covered. Built on the NVIDIA software platform incorporating CUDA, TRT, TRT-LLM, and Triton, NVIDIA NIM for LLMs brings state of the art GPU accelerated Large Language model serving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1048b66-14f3-4f47-91fd-e653979c7cd5",
   "metadata": {},
   "source": [
    "In this example we show how to deploy `LLaMa-2 7B` prebuilt and optimized model with NIM on Amazon SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa383fca-0ffb-45f9-a6cf-1849d117a386",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3686a50f-24d5-4778-a02d-28efc31373b7",
   "metadata": {},
   "source": [
    "Installs the dependencies and setup roles required to package the model and create SageMaker endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7578a7de-7ed3-4105-bec7-e5d3b04cd4bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3, json, sagemaker, time\n",
    "from sagemaker import get_execution_role\n",
    "from pathlib import Path\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role = get_execution_role()\n",
    "client = boto3.client(\"sagemaker-runtime\")\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411d6a77-cc80-4eac-b547-33b68ec5e878",
   "metadata": {},
   "source": [
    "We define the NIM image that we will be using for deploying on SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d902a31-e42b-4459-80ed-6f92ab1a5265",
   "metadata": {},
   "outputs": [],
   "source": [
    "nim_image_uri = \"<ACCOUNT>.dkr.ecr.<REGION>.amazonaws.com/nim-24.02-sm\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accb7611-819a-4be8-b4d5-c1b3af59f88f",
   "metadata": {},
   "source": [
    "### Packaging model and uploading to s3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e5b107-3240-4710-ba63-217a5a09f223",
   "metadata": {},
   "source": [
    "The prebuilt model engine we downloaded from NGC is already in `.tar.gz` format so we will upload it to S3 to later create a SageMaker Model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e2af3a-7c46-4db5-b085-5f2988ce3a72",
   "metadata": {},
   "source": [
    "Let's start. First we set the path to downloaded model `tar.gz` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8825c1e-a165-494e-ab9b-3a73e59c3258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set me if you want to try different model\n",
    "# MODEL_DOWNLOADED_PATH = <MODEL PATH>\n",
    "\n",
    "# for llama-7B example we can set\n",
    "MODEL_DOWNLOADED_PATH = 'llama-2-7b-chat_vLLAMA-2-7B-CHAT-4K-FP16-1-A100.24.02.rc2/LLAMA-2-7B-CHAT-4K-FP16-1-A100.24.02.rc2.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873003db-9b4e-4363-8fbd-a3fb28b78834",
   "metadata": {},
   "source": [
    "Next we upload it to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5360bb55-7269-48b0-9751-6cbeeec0fd2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "current_directory = Path.cwd()\n",
    "path = current_directory / MODEL_DOWNLOADED_PATH\n",
    "key_prefix = \"nim-model\"\n",
    "model_uri = sagemaker_session.upload_data(path=path, key_prefix=key_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2518de4e-dcad-4944-9025-484878edb00b",
   "metadata": {},
   "source": [
    "### Create SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d76a46-085e-4bc0-9c8e-9d633768bbcf",
   "metadata": {},
   "source": [
    "Next we can start creating a sagemaker model from the model we uploaded to s3 in the previous step.\n",
    "\n",
    "In this step we also need to provide additional Environment Variables\n",
    "- `SAGEMAKER_MODEL_NAME` which specifies the name of the model to be loaded by NIM container on SageMaker. You can provide any name, you just have to make sure it matches the name you provide in the inference request also.\n",
    "- `SAGEMAKER_NUM_GPUS` which specifies the number of GPUs the model was prebuilt to run inference on. This was specified in the name of the prebuilt model engine you downloaded from NGC. \n",
    "\n",
    "For example if the model is `LLAMA-2-7B-CHAT-4K-FP16-1-A100.24.02` then that means it's LLama 2 7B Chat model with `4K` context len, optimized for `FP16` precision and designed to run on `1 A100 GPU`.\n",
    "\n",
    "Similarly, if it's `LLAMA-2-70B-CHAT-4K-FP16-4-A100-24.02` then that means it's LLama 2 70B Chat model with `4K` context len, optimized for `FP16` precision and designed to run on `4 A100` GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3ffb98-998b-47fe-97d0-e5e80976ef48",
   "metadata": {},
   "source": [
    "Here we set model name as `llama-2-7b` and `num of GPUs` as `1` because that's what our example prebuilt engine is built for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707b5450-78cd-4c90-bfda-59141f162777",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SAGEMAKER_MODEL_NAME = \"llama-2-7b\"\n",
    "SAGEMAKER_NUM_GPUS = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b784149-2ec3-4e29-a7cf-3636843dee8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "container = {\n",
    "    \"Image\": nim_image_uri,\n",
    "    \"ModelDataUrl\": model_uri,\n",
    "    \"Environment\": {\"SAGEMAKER_MODEL_NAME\": SAGEMAKER_MODEL_NAME,\n",
    "                    \"SAGEMAKER_NUM_GPUS\": SAGEMAKER_NUM_GPUS}\n",
    "}\n",
    "sm_prefix = \"nim-model-\"\n",
    "\n",
    "sm_model_name = sm_prefix + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d658fa44-7d89-4eae-8667-d0ab8a2f47e0",
   "metadata": {},
   "source": [
    "Using the model above, we create an endpoint configuration where we can specify the type of instance we want in the endpoint. \n",
    "\n",
    "**IMPORTANT: In this case since prebuilt engine was optimized for A100 GPU we specify the `Instance Type` as `ml.p4d.24xlarge` which has A100 40GB. On other hand, `ml.p4de.24xlarge` has A100 with larger memory of 80GB so if you want to try larger LLMs like Mixtral-8x7B or LLaMa-70B or have larger batch size or larger sequence length or heavier traffic then please use `ml.p4de.24xlarge`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0af8b7c-9347-4203-aea5-f44392449f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_config_name = sm_prefix + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.p4d.24xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": 2700,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 3600\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e51121a-a662-4078-a0c6-b163cda0a718",
   "metadata": {},
   "source": [
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to InService once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75add3d0-100f-4740-b326-6f54af7e9c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = sm_prefix + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2d4bc4-b77b-4137-930e-7517295a041c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a97e4c-6dd8-4d9a-841c-e443b7c1583f",
   "metadata": {},
   "source": [
    "### Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9146f44-b85c-4125-b842-fceaf5c3cfa8",
   "metadata": {},
   "source": [
    "Once we have the endpoint's status as `InService` we can use a sample text to do a prompt text completion inference request using json as the payload format. For inference request format, currently NIM on SageMaker supports the OpenAI API completions inference protocol. For explanation of supported parameters please see [this link](https://platform.openai.com/docs/api-reference/completions/create). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57265e9-98bb-4255-ad7d-143e3aeaf9d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "  \"model\": SAGEMAKER_MODEL_NAME,\n",
    "  \"prompt\": \"The capital of France is called\",\n",
    "  \"max_tokens\": 100,\n",
    "  \"temperature\": 1,\n",
    "  \"n\": 1,\n",
    "  \"stream\": False,\n",
    "  \"stop\": [\"string\"],\n",
    "  \"frequency_penalty\": 0.0\n",
    "}\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=\"application/json\", Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "print(json.dumps(output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc21b99b-ce34-4375-9bc7-d7e8ef80f262",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Try streaming inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b89469-1f88-45d4-b103-ce2df4232037",
   "metadata": {},
   "source": [
    "NIM on SageMaker also supports streaming inference and you can enable that by setting **`\"stream\"` as `True`** in the payload and by using [`invoke_endpoint_with_response_stream`](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime/client/invoke_endpoint_with_response_stream.html) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71632bf8-5297-45db-9a92-a4a371b7da26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "  \"model\": SAGEMAKER_MODEL_NAME,\n",
    "  \"prompt\": \"The capital of France is called\",\n",
    "  \"max_tokens\": 100,\n",
    "  \"temperature\": 1,\n",
    "  \"n\": 1,\n",
    "  \"stream\": True,\n",
    "  \"stop\": [\"string\"],\n",
    "  \"frequency_penalty\": 0.0\n",
    "}\n",
    "response = client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(payload),\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/jsonlines\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0b87d4-0b86-4f26-bb69-44305825f35c",
   "metadata": {},
   "source": [
    "We do some postprocessing on the event stream to handle the streaming output tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae0af76-4013-4c72-bd0f-443aa44e6e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import LineIterator\n",
    "import re\n",
    "\n",
    "event_stream = response[\"Body\"]\n",
    "\n",
    "# Create an instance of LineIterator\n",
    "line_iterator = LineIterator(event_stream)\n",
    "\n",
    "# Iterate over the lines\n",
    "prev = None\n",
    "for line in line_iterator:\n",
    "    # Decode the line into bytes\n",
    "    decoded_line = line[len(b'data: '):].decode(\"utf-8\").rstrip('\\n')\n",
    "\n",
    "    if decoded_line == \" [DONE]\":\n",
    "        print(prev)\n",
    "        print(\"\\nStreaming Generation Finished!\")\n",
    "        break\n",
    "    else:\n",
    "        # Extract the desired information from the JSON\n",
    "        decoded_json = json.loads(decoded_line)\n",
    "        text = decoded_json['choices'][0]['text']\n",
    "        # print(text)\n",
    "        words_and_punctuations = re.findall(r\"[\\w']+|[.,!?;&()\\\"–—:;!*#@$%/\\\\<>\\[\\]{}|^~=+]\", text)# Get the last word\n",
    "        # print(words_and_punctuations[-1])\n",
    "        # print(\"===========\")\n",
    "        if len(words_and_punctuations) > 0:\n",
    "            if not has_same_prefix(prev, words_and_punctuations[-1]) and prev is not None:\n",
    "                # print(\"**************\")\n",
    "                print(prev, end=' ')\n",
    "            prev = words_and_punctuations[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19063f6-b6c0-4de2-a193-e482f26f7406",
   "metadata": {},
   "source": [
    "### Terminate endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db083f-4705-4c68-a488-f82da961be4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm.delete_model(ModelName=sm_model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
